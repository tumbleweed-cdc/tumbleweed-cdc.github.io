<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-challenges" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">6. Challenges | Tumbleweed</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://tumbleweed-cdc.github.io/img/FaviconTumbleweedTransparent.ico"><meta data-rh="true" name="twitter:image" content="https://tumbleweed-cdc.github.io/img/FaviconTumbleweedTransparent.ico"><meta data-rh="true" property="og:url" content="https://tumbleweed-cdc.github.io/docs/challenges"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="6. Challenges | Tumbleweed"><meta data-rh="true" name="description" content="During the development and deployment phases of Tumbleweed, our team faced several technical challenges regarding consumer data consumption, pipeline auto-deployment, Tumbleweed component configuration, database management, and application security."><meta data-rh="true" property="og:description" content="During the development and deployment phases of Tumbleweed, our team faced several technical challenges regarding consumer data consumption, pipeline auto-deployment, Tumbleweed component configuration, database management, and application security."><link data-rh="true" rel="icon" href="/img/tumbleweed_favicon.ico"><link data-rh="true" rel="canonical" href="https://tumbleweed-cdc.github.io/docs/challenges"><link data-rh="true" rel="alternate" href="https://tumbleweed-cdc.github.io/docs/challenges" hreflang="en"><link data-rh="true" rel="alternate" href="https://tumbleweed-cdc.github.io/docs/challenges" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.6cfd2f41.css">
<script src="/assets/js/runtime~main.e81ce6e8.js" defer="defer"></script>
<script src="/assets/js/main.9d8f4acc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/FaviconTumbleweedTransparent.ico" alt="Tumbleweed Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/FaviconTumbleweedTransparent.ico" alt="Tumbleweed Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Tumbleweed</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/introduction">CASE STUDY</a><a class="navbar__item navbar__link" href="/#team">TEAM</a><a href="https://github.com/tumbleweed-cdc" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GITHUB<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/introduction">1. Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/problem_domain">2. Problem Domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/existing_solutions">3. Existing Solutions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tumbleweed">4. Tumbleweed</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/using_tumbleweed">5. Using Tumbleweed</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/challenges">6. Challenges</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/future_work">7. Future Work</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">6. Challenges</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>6. Challenges</h1></header>
<p>During the development and deployment phases of Tumbleweed, our team faced several technical challenges regarding consumer data consumption, pipeline auto-deployment, Tumbleweed component configuration, database management, and application security.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-consumer-service-data-consumption">6.1 Consumer Service Data Consumption<a href="#61-consumer-service-data-consumption" class="hash-link" aria-label="Direct link to 6.1 Consumer Service Data Consumption" title="Direct link to 6.1 Consumer Service Data Consumption">​</a></h2>
<p>Although Debezium maintains a source connector which is a good fit for Tumbleweeds use case, it does not provide an appropriate sink connector for microservices on the consumer end. Without a sink connector built into Tumbleweed, anyone who wished to integrate our pipeline into their architecture would be required to extensively alter their consumer codebases, greatly increasing the level of user complexity. They would also be required to find a Kafka client which matched the language of their codebase, before they could even implement message consumption. In addition, the consumer would also have to implement error handling and more complex message processing. Finally, this would require public access to the Kafka cluster containers which would be an increased security risk.</p>
<p>It seemed logical for Tumbleweed to create a custom sink connector for our pipeline. After researching, we found KafkaJS, a Node.JS Kafka client. This allowed us to integrate Kafka communication into our architecture. KafkaJS was used to create the connection between Kafka and the consumers, monitoring topics specified by the user and streaming incoming messages to a provided Tumbleweed endpoint via Server Sent Events (SSE).</p>
<figure><img src="/img/sse.png" class="Server Sent Events" alt="Server Sent Events" width="80%"><figcaption>Figure 1: Server Sent Events.</figcaption></figure>
<p>SSE creates a unidirectional long-lived HTTP connection from server to client, allowing us to push messages to consumers in real time, which is ideal for the goals of our application. In order for a consumer service to receive these messages, only a small amount of code is required in their codebase, and this can be written in any language that supports SSE.</p>
<p>Two other options we explored were the use of websockets or polling. Websockets are a communication protocol that allow for real-time, bi-directional communication between a client and a server. The connection between client and server is persistent; once the connection is established, data can be transmitted between the client and server in real time. While the real-time communication capabilities of websockets suited our needs, we did not need bi-directional communication.</p>
<figure><img src="/img/websockets.png" class="Websockets" alt="Websockets" width="80%"><figcaption>Figure 2: Websockets.</figcaption></figure>
<p>Polling is another method that can be used for real-time communication between servers and clients. There are two different types of polling – short and long. Short polling involves a client repeatedly sending HTTP requests in regular intervals to the server to check for new data. The advantage to using short polling is that it’s simple to implement on both the client and server side, and is ideal for requests that take a long time to execute, as it allows for asynchronous processing. The drawbacks are that short polling can result in excessive network requests, leading to increased server loads and network traffic. Additionally, updated data is only received during the polling intervals; the user does not receive their data as quickly as compared to other solutions.</p>
<figure><img src="/img/short_polling.png" class="Short-polling" alt="Short-polling" width="80%"><figcaption>Figure 3: Short-Polling.</figcaption></figure>
<p>Long polling differs from short polling in that it keeps the connection open until new data arrives. Once the client receives the response/data, a new request is sent either immediately, or after a predetermined interval to establish a new connection. The advantage again lies in its simple implementation. However, reliable message ordering is not guaranteed due to the possibility of multiple HTTP requests from the same client to be in flight simultaneously<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>. There is also higher latency due to the need to reopen connections, and the client having to wait for a server response. Finally, both forms of polling may face rate limiting issues if trying to receive the high volume of frequent responses which Tumbleweed would need to handle.</p>
<figure><img src="/img/long_polling.png" class="Long-polling" alt="Long-polling" width="80%"><figcaption>Figure 4: Long-Polling.</figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-terraform-deployment-with-multi-node-kafka-cluster">6.2 Terraform Deployment with Multi-Node Kafka Cluster<a href="#62-terraform-deployment-with-multi-node-kafka-cluster" class="hash-link" aria-label="Direct link to 6.2 Terraform Deployment with Multi-Node Kafka Cluster" title="Direct link to 6.2 Terraform Deployment with Multi-Node Kafka Cluster">​</a></h2>
<p>When writing the configuration files for Terraform and deploying the Tumbleweed infrastructure, we ran into issues with the Kafka brokers, Kafka controllers, Connect and the Tumbleweed backend API not being able to communicate with each other. This miscommunication resulted in the services not being able to reach a stable running state.</p>
<p>In the early versions and development stages of Tumbleweed, we used Docker to run our Kafka containers, and for the most part, inter-container communication was abstracted away from us, due to the fact it was running on our local machines and using Docker&#x27;s default networking settings. Transitioning to ECS deployment introduced new challenges in enabling containers to communicate securely, while also restricting external access to services that did not need to be publicly exposed.</p>
<p>To resolve these issues, we used Amazon ECS Service Discovery to provide DNS hostnames for each of our services, allowing simplified service-to-service communication in our ECS cluster. We also utilized a VPC (Virtual Private Cloud), a NAT (Network Address Translation), an Internet Gateway, security groups and routing tables to manage internal and external communication.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="63-multiple-pipelines-sharing-a-single-replication-slot-in-development">6.3 Multiple Pipelines Sharing a Single Replication Slot in Development<a href="#63-multiple-pipelines-sharing-a-single-replication-slot-in-development" class="hash-link" aria-label="Direct link to 6.3 Multiple Pipelines Sharing a Single Replication Slot in Development" title="Direct link to 6.3 Multiple Pipelines Sharing a Single Replication Slot in Development">​</a></h2>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>What is a replication slot?</div><div class="admonitionContent_BuS1"><p>A replication slot is a feature in Postgres that provides a robust way to handle data replication between a primary database server and one or more consumers, such as secondary Postgres servers, CDC tools, or other downstream systems<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. These replication slots ensure that no data is lost by keeping necessary WAL (Write-Ahead Log) files until all consumers have confirmed they have received that data <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>.  We can minimize performance impact on the database while maintaining consistent data change order by reading from the WAL instead of querying the database directly.</p></div></div>
<p>The production version of Tumbleweed is meant to be run as a single pipeline instance for a full architecture of services, with a single source connector for each producer service. During the early development stages of Tumbleweed, each member of our team had their own instance of Tumbleweed running, but these individual instances shared the same replication slot on the same AWS RDS source database.</p>
<p>A PostgreSQL replication slot will only emit changes once<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>. If those changes are captured by a single pipeline, they can be later streamed to any number of downstream consumers. However, as our team was running multiple pipeline instances, these changes were only captured by a single instance, and therefore only the consumers of that instance would receive data.</p>
<figure><img src="/img/shared_replication_slot.png" class="Sharing replication slot" alt="Sharing replication slot" width="80%"><figcaption>Figure 5: Multiple instances sharing a single replication slot.</figcaption></figure>
<p>The solution to this issue in development was to create a unique replication slot name for each connector. This replication slot name was based on the connector details that the user provides when creating a new source. In doing so, we were able to ensure that each developer was able to connect to the same source database for testing, and still consume the data.</p>
<p>While this issue was related to development of Tumbleweed, it should not arise in production. However, to make sure that such a scenario cannot be introduced unintentionally, the production version allows for only a single uniquely-named replication slot per source connector.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="64-managing-wal-disk-size-growth-in-postgresql-on-aws-rds">6.4 Managing WAL Disk Size Growth in PostgreSQL on AWS RDS<a href="#64-managing-wal-disk-size-growth-in-postgresql-on-aws-rds" class="hash-link" aria-label="Direct link to 6.4 Managing WAL Disk Size Growth in PostgreSQL on AWS RDS" title="Direct link to 6.4 Managing WAL Disk Size Growth in PostgreSQL on AWS RDS">​</a></h2>
<p>When a replication consumer goes offline, its PostgreSQL replication slot becomes inactive. When this occurs, Postgres will retain all WAL segments after the latest LSN (log sequence number) for unconsumed changes. The LSN is an unsigned 64-bit integer used to determine a position in the WAL<sup><a href="#user-content-fn-2" id="user-content-fnref-2-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. These segments are small, 16MB by default, but can be configured to larger sizes. Tumbleweed needs to retain WAL segments as it uses Debezium, which relies on the WAL to capture changes. If a consumer goes offline and the WAL segments are not retained, changes made while the consumer is offline will be lost.</p>
<p>During development, we used AWS RDS to spin up a Postgres database to test database connectors and data consumption, and we encountered a challenge with uncontrolled growth in WAL disk size. RDS writes to a heartbeat table in the internal “rdsadmin” database every 5 minutes. Even when the database appears to be idle, this generates traffic. Additionally, in AWS, RDS has increased WAL segments from 16MB to 64MB in size<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>. Thus these periodic writes to the heartbeat table within the “rdsadmin” database trigger a new WAL segment of 64MB to be created. If the replication slot remains inactive and the LSN doesn&#x27;t advance, Postgres will continue to retain these segments. This causes the quick consumption of disk space, potentially causing the database to crash.</p>
<p>We explored several solutions to address this issue:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring-and-removing-inactive-slots">Monitoring and removing inactive slots<a href="#monitoring-and-removing-inactive-slots" class="hash-link" aria-label="Direct link to Monitoring and removing inactive slots" title="Direct link to Monitoring and removing inactive slots">​</a></h3>
<p>The first solution was to manually remove any Debezium connectors that were no longer being used along with its replication slot from the source database. While this solution was effective, it required frequent monitoring of RDS disk usage and replication slot activity. This was prone to human error, occasionally allowing WAL growth to spiral out of control.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="creating-and-writing-to-a-heartbeat-table">Creating and writing to a heartbeat table<a href="#creating-and-writing-to-a-heartbeat-table" class="hash-link" aria-label="Direct link to Creating and writing to a heartbeat table" title="Direct link to Creating and writing to a heartbeat table">​</a></h3>
<p>Our second solution was to create a heartbeat table in the source database and have Debezium write to it in five-minute intervals, keeping the database and its replication slots active. This kept replication slots active, and prevented WAL segments from accumulating.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="removing-the-inactive-slot-on-connector-deletion">Removing the inactive slot on connector deletion<a href="#removing-the-inactive-slot-on-connector-deletion" class="hash-link" aria-label="Direct link to Removing the inactive slot on connector deletion" title="Direct link to Removing the inactive slot on connector deletion">​</a></h3>
<p>The third and final solution was to create the heartbeat table as in the second solution above, while also removing replication slots when its connector to the source database is removed. This seemed to be the optimal solution as it does not require frequent monitoring of the WAL size. It also ensures the WAL remains at a consistent size, between 100MB to 200MB.</p>
<p>While this solution controlled the growth of the WAL disk size, we recognize that this may not be an issue in production. The source databases users will be providing will more than likely be in a continuously active state, i.e., data being written and messages being consumed frequently. However, in the event that this was not the case or for pre-production testing of a user&#x27;s services with a Tumbleweed pipeline, we aimed to provide a solution that prevented the WAL disk size from growing out of control and crashing their database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="65-securing-tumbleweed">6.5 Securing Tumbleweed<a href="#65-securing-tumbleweed" class="hash-link" aria-label="Direct link to 6.5 Securing Tumbleweed" title="Direct link to 6.5 Securing Tumbleweed">​</a></h2>
<p>Security is important when building an application that manages and interacts with a user’s data. Tumbleweed is self-hosted and auto-deployed to AWS ECS on the user’s AWS account. Each user spins up their own instance of Tumbleweed which is accessed via a public IP provided by ECS.</p>
<p>This deployment model introduced security challenges. Because the users access Tumbleweed through a public IP address, our primary focus was on preventing unauthorized access to the pipeline’s data and infrastructure. Securing Tumbleweed required implementing robust measures to protect communication channels, and control access through whitelisted IPs, IAM roles and policies.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="whitelisting-ips-for-controlled-access-to-pipeline-ui">Whitelisting IPs for Controlled Access to Pipeline UI<a href="#whitelisting-ips-for-controlled-access-to-pipeline-ui" class="hash-link" aria-label="Direct link to Whitelisting IPs for Controlled Access to Pipeline UI" title="Direct link to Whitelisting IPs for Controlled Access to Pipeline UI">​</a></h3>
<figure><img src="/img/whitelist_ips.png" class="Whitelist ips" alt="Whitelist ips" width="80%"><figcaption>Figure 7: Whitelisting IP&#x27;s to controll access.</figcaption></figure>
<p>During Tumbleweed’s installation, the user is prompted to provide a list of IP addresses to whitelist. These IP addresses are stored in a Terraform variable, which is then used to configure an ECS security group, granting controlled access to the application. This is done through creating inbound rules that allow traffic only from the specified IP addresses. By doing this, we ensure that access to the application is tightly controlled and limited to trusted resources.</p>
<p>While this does secure the application, the tradeoff of this design is that the user must find out the IP address for each machine that intends on accessing the application’s UI. This process can be tedious, especially in environments where the public IP address is not readily available.</p>
<p>Additionally, Tumbleweed currently lacks the ability to alter the whitelist after deployment without complete teardown and redeployment. This means that users are essentially stuck with the IP address input at deploy time which can become burdensome over time, particularly for users with a large number of machines or for those operating in cloud environments where instances are frequently spun up and down. While whitelisting provides robust security, it may not be the most user-friendly approach for all scenarios.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="controlling-access-through-security-groups">Controlling Access Through Security Groups<a href="#controlling-access-through-security-groups" class="hash-link" aria-label="Direct link to Controlling Access Through Security Groups" title="Direct link to Controlling Access Through Security Groups">​</a></h3>
<p>For public access control, public facing Tumbleweed Backend API ports were routed through the Internet Gateway, with access being controlled by specific security group ingress rules. For private access control, outgoing traffic from Connect and Debezium in our private subnet was routed through the NAT gateway, allowing contact from that service to user source databases but isolating it from other uninitiated direct internet access. Inter-service communication was made possible by placing the services within a VPC, with its security group having unrestricted ingress rules for a private CIDR block. In doing so, we were able to prevent direct internet access to the containers which do not require such access.</p>
<hr>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_LWe7 sr-only" id="footnote-label">Footnotes<a href="#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1">
<p><a href="https://ably.com/blog/websockets-vs-long-polling" target="_blank" rel="noopener noreferrer">K. Kilbride-Singh, “Long Polling vs WebSockets - which to use in 2024,” Ably Realtime, Dec. 21, 2023.</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://medium.com/@LadyNoBug/replication-slot-in-postgres-5059527a9e69" target="_blank" rel="noopener noreferrer">“Replication slot in Postgres - Ladynobug - Medium,” Medium, Jan. 04, 2022</a> <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a> <a href="#user-content-fnref-2-2" data-footnote-backref="" aria-label="Back to reference 2-2" class="data-footnote-backref">↩<sup>2</sup></a></p>
</li>
<li id="user-content-fn-3">
<p><a href="https://www.dragonflydb.io/faq/postgresql-replication-slots" target="_blank" rel="noopener noreferrer">“What are PostgreSQL replication slots and how do they work?,” DragonflyDB.</a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://www.postgresql.org/docs/17/logicaldecoding-explanation.html" target="_blank" rel="noopener noreferrer">“47.2. Logical decoding concepts,” PostgreSQL Documentation, Nov. 21, 2024</a> <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5">
<p><a href="https://www.morling.dev/blog/insatiable-postgres-replication-slot/" target="_blank" rel="noopener noreferrer">G. Morling, “The insatiable postgres replication slot,” Gunnar Morling Blog, Nov. 30, 2022.</a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/using_tumbleweed"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">5. Using Tumbleweed</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/future_work"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">7. Future Work</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#61-consumer-service-data-consumption" class="table-of-contents__link toc-highlight">6.1 Consumer Service Data Consumption</a></li><li><a href="#62-terraform-deployment-with-multi-node-kafka-cluster" class="table-of-contents__link toc-highlight">6.2 Terraform Deployment with Multi-Node Kafka Cluster</a></li><li><a href="#63-multiple-pipelines-sharing-a-single-replication-slot-in-development" class="table-of-contents__link toc-highlight">6.3 Multiple Pipelines Sharing a Single Replication Slot in Development</a></li><li><a href="#64-managing-wal-disk-size-growth-in-postgresql-on-aws-rds" class="table-of-contents__link toc-highlight">6.4 Managing WAL Disk Size Growth in PostgreSQL on AWS RDS</a><ul><li><a href="#monitoring-and-removing-inactive-slots" class="table-of-contents__link toc-highlight">Monitoring and removing inactive slots</a></li><li><a href="#creating-and-writing-to-a-heartbeat-table" class="table-of-contents__link toc-highlight">Creating and writing to a heartbeat table</a></li><li><a href="#removing-the-inactive-slot-on-connector-deletion" class="table-of-contents__link toc-highlight">Removing the inactive slot on connector deletion</a></li></ul></li><li><a href="#65-securing-tumbleweed" class="table-of-contents__link toc-highlight">6.5 Securing Tumbleweed</a><ul><li><a href="#whitelisting-ips-for-controlled-access-to-pipeline-ui" class="table-of-contents__link toc-highlight">Whitelisting IPs for Controlled Access to Pipeline UI</a></li><li><a href="#controlling-access-through-security-groups" class="table-of-contents__link toc-highlight">Controlling Access Through Security Groups</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>